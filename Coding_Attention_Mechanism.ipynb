{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5bed801",
   "metadata": {},
   "source": [
    "**A simple self-attention mechanism without trainable weights**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f52f04e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "inputs = torch.tensor(\n",
    "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
    "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
    "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
    "   [0.22, 0.58, 0.33], # with     (x^4)\n",
    "   [0.77, 0.25, 0.10], # one      (x^5)\n",
    "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68b86062",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5500, 0.8700, 0.6600])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_query = inputs[1]\n",
    "input_query "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0beb321b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4300, 0.1500, 0.8900])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_1 = inputs[0]\n",
    "input_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb07bee0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9544)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.dot(input_query, input_1) #dot product calculates the similarity between the query and the key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6fd0dda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = inputs[1] #2nd input is taken as example and this is called as query\n",
    "\n",
    "attn_scores_2 = torch.empty(inputs.shape[0]) #empty tensor is created to store the attention score of the query token\n",
    "\n",
    "for i, x_i in enumerate(inputs): #enumerate function is used to get the index and the value of the input tensor\n",
    "    attn_scores_2[i] = torch.dot(query, x_i) #dot product is calculated between the query and the key (key refers to other tokens including the query token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2e6b947",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_scores_2 #this is the attention score of the query token attention scores refers to the similarity between the query and the key if you see the query token is most similar to the query token itself that is why it is the highest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85f2d533",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weights_2 = torch.nn.functional.softmax(attn_scores_2, dim=0) #softmax is applied to the attention scores to get the attention weights\n",
    "attn_weights_2 #this is the attention weight of the query token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0999fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we will calculate the context vector of the query token which is the weighted sum of the input tokens with this attention weights\n",
    "\n",
    "query = inputs[1]\n",
    "\n",
    "context_vec_2 = torch.zeros(query.shape) #empty tensor is created to store the context vector of the query token and the shape of the context vector is the same as the query token\n",
    "\n",
    "for i, x_i in enumerate(inputs): #enumerate function is used to get the index and the value of the input tensor\n",
    "    context_vec_2 += attn_weights_2[i] * x_i #weighted sum is calculated between the attention weights and the input tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "71492bde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4419, 0.6515, 0.5683])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_vec_2 #this is the context vector of the query token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8c5aff9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_scores = torch.zeros(6,6) #empty tensor is created to store the attention scores of the all the token with respect to all the other tokens so it's a 6x6 matrix\n",
    "\n",
    "for i, x_i in enumerate(inputs): #enumerate function is used to get the index and the value of the input tensor\n",
    "    for j, x_j in enumerate(inputs): #enumerate function is used to get the index and the value of the input tensor\n",
    "        attn_scores[i, j] = torch.dot(x_i, x_j) #dot product is calculated between the input tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "50d4662f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
       "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
       "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
       "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
       "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
       "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_scores #if you look at this you will see that attn_scores_2 is the second row of this matrix so each row is the attention score of that token with respect to all the other tokens and the diagonal elements are the attention scores of the query token with respect to itself which is the highest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5302d9c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
       "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
       "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
       "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
       "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
       "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_scores = inputs @ inputs.T #this is the same as the above code but it is more efficient and it is a matrix multiplication\n",
    "attn_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "206e0d5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n",
       "        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n",
       "        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n",
       "        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n",
       "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n",
       "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weights = torch.softmax(attn_scores, dim=1) #softmax is applied to the attention scores to get the attention weights dim=1 means that the softmax is applied to each row of the matrix so each row adds up to 1\n",
    "\n",
    "attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1a0cb141",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4421, 0.5931, 0.5790],\n",
       "        [0.4419, 0.6515, 0.5683],\n",
       "        [0.4431, 0.6496, 0.5671],\n",
       "        [0.4304, 0.6298, 0.5510],\n",
       "        [0.4671, 0.5910, 0.5266],\n",
       "        [0.4177, 0.6503, 0.5645]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_vec = attn_weights @ inputs #context vector is the weighted sum of the input tokens with this attention weights\n",
    "context_vec #each row of the context vector of that token "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ab57ab",
   "metadata": {},
   "source": [
    "**Implementing self attention with trainable weights**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2a63b383",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_2 = inputs[1]\n",
    "d_in = inputs.shape[1]\n",
    "d_out = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "805ac397",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5500, 0.8700, 0.6600])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "71425ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "W_q = torch.nn.Parameter(torch.rand(d_in, d_out))\n",
    "W_k = torch.nn.Parameter(torch.rand(d_in, d_out))\n",
    "W_v = torch.nn.Parameter(torch.rand(d_in, d_out))\n",
    "\n",
    "# these are trainable weights for the query, key and value matrices we can choose d_out to be any value we want that is the dimension of the output space for simplicity we will choose it to be 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3afcd08c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[0.2961, 0.5166],\n",
       "        [0.2517, 0.6886],\n",
       "        [0.0740, 0.8665]], requires_grad=True)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c395fca8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[0.1366, 0.1025],\n",
       "        [0.1841, 0.7264],\n",
       "        [0.3153, 0.6871]], requires_grad=True)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1a87d3ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[0.0756, 0.1966],\n",
       "        [0.3164, 0.4017],\n",
       "        [0.1186, 0.8274]], requires_grad=True)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "48753eba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4306, 1.4551], grad_fn=<SqueezeBackward4>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_2 = x_2 @ W_q #this is the query vector of the 2nd token\n",
    "query_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3aef8580",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4433, 1.1419], grad_fn=<SqueezeBackward4>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key_2 = x_2 @ W_k #this is the key vector of the 2nd token\n",
    "key_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f6d1b19d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.3951, 1.0037], grad_fn=<SqueezeBackward4>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value_2 = x_2 @ W_v #this is the value vector of the 2nd token\n",
    "value_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "26a53945",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to calculate the context vector for 2nd token we will use the query vector of the 2nd token and the key vectors of all the other tokens and the value vectors of all the other tokens \n",
    "# so instead of calculating the key and value vectors seperately we will calculate them in one go by using the matrix multiplication\n",
    "\n",
    "keys = inputs @ W_k #this is the key matrix of all the tokens\n",
    "values = inputs @ W_v #this is the value matrix of all the tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0db3be70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3669, 0.7646],\n",
       "        [0.4433, 1.1419],\n",
       "        [0.4361, 1.1156],\n",
       "        [0.2408, 0.6706],\n",
       "        [0.1827, 0.3292],\n",
       "        [0.3275, 0.9642]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys #2nd row of the key matrix is the key vector of the 2nd token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e4d8405a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1855, 0.8812],\n",
       "        [0.3951, 1.0037],\n",
       "        [0.3879, 0.9831],\n",
       "        [0.2393, 0.5493],\n",
       "        [0.1492, 0.3346],\n",
       "        [0.3221, 0.7863]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values #2nd row of the value matrix is the value vector of the 2nd token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "576c18fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.8524, grad_fn=<DotBackward0>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_score_22 = torch.dot(query_2, key_2) #this is the attention score of the 2nd token with respect to itself\n",
    "attn_score_22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f7d85555",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440],\n",
       "       grad_fn=<SqueezeBackward4>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_scores_2 = query_2 @ keys.T #this is the attention score of the 2nd token with respect to all the other tokens look at the 2nd value of the attn_scores_2 tensor it is the same as the attn_score_22 tensor\n",
    "attn_scores_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f137107c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_k = keys.shape[1] #this is the dimension of the key vector \n",
    "d_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7f67184d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1500, 0.2264, 0.2199, 0.1311, 0.0906, 0.1820],\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weights_2 = torch.softmax(attn_scores_2 / torch.sqrt(torch.tensor(d_k)), dim=0) \n",
    "attn_weights_2\n",
    "\n",
    "#this is the attention weight of the 2nd token we divide the attention scores by the square root of the dimension of the key vector to prevent the attention scores from becoming too large and this was recommended by the authors of the paper attention is all you need\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cf96927b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.3061, 0.8210], grad_fn=<SqueezeBackward4>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_vec_2 = attn_weights_2 @ values #this is the context vector of the 2nd token\n",
    "context_vec_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dca3771",
   "metadata": {},
   "source": [
    "**Implementing a Compact Self Attention Class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f8c966ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SelfAttention()"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class SelfAttention(torch.nn.Module):\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        self.W_q = torch.nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_k = torch.nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_v = torch.nn.Parameter(torch.rand(d_in, d_out))\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        queries = x @ self.W_q\n",
    "        keys = x @ self.W_k\n",
    "        values = x @ self.W_v\n",
    "\n",
    "        attn_scores = queries @ keys.T\n",
    "        attn_weights = torch.softmax(attn_scores / torch.sqrt(torch.tensor(keys.shape[1])), dim=1)\n",
    "        context_vec = attn_weights @ values\n",
    "\n",
    "        return context_vec\n",
    "\n",
    "\n",
    "torch.manual_seed(123)\n",
    "sa_v1 = SelfAttention(d_in=3, d_out=2)\n",
    "sa_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3dc6430e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4300, 0.1500, 0.8900],\n",
       "        [0.5500, 0.8700, 0.6600],\n",
       "        [0.5700, 0.8500, 0.6400],\n",
       "        [0.2200, 0.5800, 0.3300],\n",
       "        [0.7700, 0.2500, 0.1000],\n",
       "        [0.0500, 0.8000, 0.5500]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7f828936",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2996, 0.8053],\n",
       "        [0.3061, 0.8210],\n",
       "        [0.3058, 0.8203],\n",
       "        [0.2948, 0.7939],\n",
       "        [0.2927, 0.7891],\n",
       "        [0.2990, 0.8040]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sa_v1(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5627c903",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention_v2(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        self.W_q = torch.nn.Linear(d_in, d_out, bias=False) # we use Linear because instead of random this method has a better weight initilization scheme\n",
    "        self.W_k = torch.nn.Linear(d_in, d_out, bias=False)\n",
    "        self.W_v = torch.nn.Linear(d_in, d_out, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        queries = self.W_q(inputs)\n",
    "        keys = self.W_k(inputs)\n",
    "        values = self.W_v(inputs)\n",
    "\n",
    "        attn_scores = queries @ keys.T\n",
    "        attn_weights = torch.softmax(attn_scores / torch.sqrt(torch.tensor(keys.shape[1])), dim=1)\n",
    "        context_vec = attn_weights @ values\n",
    "\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ab74ae6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 2)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_in, d_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "13464b7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SelfAttention_v2(\n",
       "  (W_q): Linear(in_features=3, out_features=2, bias=False)\n",
       "  (W_k): Linear(in_features=3, out_features=2, bias=False)\n",
       "  (W_v): Linear(in_features=3, out_features=2, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "sa_v2 = SelfAttention_v2(d_in, d_out)\n",
    "sa_v2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d5bc4e7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5337, -0.1051],\n",
       "        [-0.5323, -0.1080],\n",
       "        [-0.5323, -0.1079],\n",
       "        [-0.5297, -0.1076],\n",
       "        [-0.5311, -0.1066],\n",
       "        [-0.5299, -0.1081]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sa_v2(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f933ef78",
   "metadata": {},
   "source": [
    "**Applying a causal attention mask**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c0f566bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's say the LLM generates one word at a time so in the example sentense \"Your Journey Starts with One step\"\n",
    "# when giving the context vector of \"Your\" the LLM should only have access to that token only and when given the context vector of \"Your Journey\" the LLM should not have access to other context tokens\n",
    "# just because it is in our training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "88f072e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3536,  0.3965],\n",
       "        [-0.3021, -0.0289],\n",
       "        [-0.3015, -0.0232],\n",
       "        [-0.1353, -0.0978],\n",
       "        [-0.2052,  0.0870],\n",
       "        [-0.1542, -0.1499]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries = sa_v2.W_q(inputs)\n",
    "queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f76ca47b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5740,  0.2727],\n",
       "        [-0.8709,  0.1008],\n",
       "        [-0.8628,  0.1060],\n",
       "        [-0.4789,  0.0051],\n",
       "        [-0.4744,  0.1696],\n",
       "        [-0.5888, -0.0388]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys = sa_v2.W_k(inputs)\n",
    "keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d25b033a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5740,  0.2727],\n",
       "        [-0.8709,  0.1008],\n",
       "        [-0.8628,  0.1060],\n",
       "        [-0.4789,  0.0051],\n",
       "        [-0.4744,  0.1696],\n",
       "        [-0.5888, -0.0388]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values = sa_v2.W_v(inputs)\n",
    "keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "284e0e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_scores = queries @ keys.T\n",
    "attn_weights = torch.softmax(attn_scores / torch.sqrt(torch.tensor(keys.shape[1])), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e9493ea8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1717, 0.1762, 0.1761, 0.1555, 0.1627, 0.1579],\n",
       "        [0.1636, 0.1749, 0.1746, 0.1612, 0.1605, 0.1652],\n",
       "        [0.1637, 0.1749, 0.1746, 0.1611, 0.1606, 0.1651],\n",
       "        [0.1636, 0.1704, 0.1702, 0.1652, 0.1632, 0.1674],\n",
       "        [0.1667, 0.1722, 0.1721, 0.1618, 0.1633, 0.1639],\n",
       "        [0.1624, 0.1709, 0.1706, 0.1654, 0.1625, 0.1682]],\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b3737d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_length = attn_scores.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2fff2cf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 0.],\n",
       "        [1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_simple = torch.tril(torch.ones(context_length, context_length))\n",
    "mask_simple # creates a mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f1e05b87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1717, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1636, 0.1749, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1637, 0.1749, 0.1746, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1636, 0.1704, 0.1702, 0.1652, 0.0000, 0.0000],\n",
       "        [0.1667, 0.1722, 0.1721, 0.1618, 0.1633, 0.0000],\n",
       "        [0.1624, 0.1709, 0.1706, 0.1654, 0.1625, 0.1682]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_attn_weights = attn_weights * mask_simple\n",
    "masked_attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "58c87d1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1717],\n",
       "        [0.3385],\n",
       "        [0.5132],\n",
       "        [0.6693],\n",
       "        [0.8361],\n",
       "        [1.0000]], grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row_sums = masked_attn_weights.sum(dim=1,keepdim=True)\n",
    "row_sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f77a78c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.4833, 0.5167, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3190, 0.3408, 0.3402, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2445, 0.2545, 0.2542, 0.2468, 0.0000, 0.0000],\n",
       "        [0.1994, 0.2060, 0.2058, 0.1935, 0.1953, 0.0000],\n",
       "        [0.1624, 0.1709, 0.1706, 0.1654, 0.1625, 0.1682]],\n",
       "       grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_norm_attn_weights = masked_attn_weights/row_sums\n",
    "masked_norm_attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "65fc5f74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 1., 1., 1., 1.],\n",
       "        [0., 0., 1., 1., 1., 1.],\n",
       "        [0., 0., 0., 1., 1., 1.],\n",
       "        [0., 0., 0., 0., 1., 1.],\n",
       "        [0., 0., 0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# instead of getting the attention score then normalizing to get attention weights and then getting the masked attention weights and then again normalizing what we can do is get the attention score calculate the masked attention score and then calculate the normalized version\n",
    "\n",
    "mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9d5798a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3111,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
       "        [0.1655, 0.2602,   -inf,   -inf,   -inf,   -inf],\n",
       "        [0.1667, 0.2602, 0.2577,   -inf,   -inf,   -inf],\n",
       "        [0.0510, 0.1080, 0.1064, 0.0643,   -inf,   -inf],\n",
       "        [0.1415, 0.1875, 0.1863, 0.0987, 0.1121,   -inf],\n",
       "        [0.0476, 0.1192, 0.1171, 0.0731, 0.0477, 0.0966]],\n",
       "       grad_fn=<MaskedFillBackward0>)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked = attn_scores.masked_fill(mask.bool(), -torch.inf)\n",
    "masked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b8a93e25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.4833, 0.5167, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3190, 0.3408, 0.3402, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2445, 0.2545, 0.2542, 0.2468, 0.0000, 0.0000],\n",
       "        [0.1994, 0.2060, 0.2058, 0.1935, 0.1953, 0.0000],\n",
       "        [0.1624, 0.1709, 0.1706, 0.1654, 0.1625, 0.1682]],\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weights = torch.softmax(masked/torch.sqrt(torch.tensor(keys.shape[1])), dim=1)\n",
    "attn_weights # this is same as what we got before"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04cc4ece",
   "metadata": {},
   "source": [
    "**Masking additional attention weights with dropout**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "14247c2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dropout(p=0.5, inplace=False)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#masking will drop random position from our attn_weight so model will reduce overfitting\n",
    "drop_layer = torch.nn.Dropout(0.5)\n",
    "drop_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f11d23bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = torch.ones(6,6)\n",
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2e7e2ada",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2., 0., 2., 2., 2., 2.],\n",
       "        [2., 0., 2., 0., 2., 2.],\n",
       "        [0., 0., 2., 0., 2., 0.],\n",
       "        [2., 0., 2., 0., 2., 2.],\n",
       "        [2., 2., 0., 2., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 2.]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drop_layer(example) # Other values become bigger because PyTorch rescales them to keep the average unchanged."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d22cd9",
   "metadata": {},
   "source": [
    "**create a final class of everything we learned causal self attention plus dropout**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "40060550",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CausalAttention(\n",
       "  (W_q): Linear(in_features=3, out_features=2, bias=False)\n",
       "  (W_k): Linear(in_features=3, out_features=2, bias=False)\n",
       "  (W_v): Linear(in_features=3, out_features=2, bias=False)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class CausalAttention(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out, context_length, dropout):\n",
    "        super().__init__()\n",
    "        self.W_q = torch.nn.Linear(d_in, d_out, bias=False)\n",
    "        self.W_k = torch.nn.Linear(d_in, d_out, bias=False)\n",
    "        self.W_v = torch.nn.Linear(d_in, d_out, bias=False)\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1)) #this is a buffer so it is not a parameter and it is not updated during the training process and when we train with GPU it will be moved to the GPU\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape #b is the batch size, num_tokens is the number of tokens in the input, d_in is the dimension of the input\n",
    "        queries = self.W_q(x)\n",
    "        keys = self.W_k(x)\n",
    "        values = self.W_v(x)\n",
    "        attn_scores = queries @ keys.transpose(1, 2) #transpose is used to make the keys matrix a 3d tensor and why we do that because we want to make the dot product of the queries and the keys and we want to do it for all the tokens in the input so we need to make the keys matrix a 3d tensor\n",
    "        attn_scores.masked_fill_(self.mask.bool()[:num_tokens, :num_tokens], -torch.inf) #this is the mask that we created earlier the causal mask\n",
    "        attn_weights = torch.softmax(attn_scores / torch.sqrt(torch.tensor(keys.shape[1])), dim=1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec\n",
    "\n",
    "\n",
    "torch.manual_seed(123)\n",
    "ca = CausalAttention(d_in=3, d_out=2, context_length=6, dropout=0.5)\n",
    "ca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a6e97fbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 6, 3])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = torch.stack((inputs, inputs), dim=0)\n",
    "batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f1cbe40e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.4300, 0.1500, 0.8900],\n",
       "         [0.5500, 0.8700, 0.6600],\n",
       "         [0.5700, 0.8500, 0.6400],\n",
       "         [0.2200, 0.5800, 0.3300],\n",
       "         [0.7700, 0.2500, 0.1000],\n",
       "         [0.0500, 0.8000, 0.5500]],\n",
       "\n",
       "        [[0.4300, 0.1500, 0.8900],\n",
       "         [0.5500, 0.8700, 0.6600],\n",
       "         [0.5700, 0.8500, 0.6400],\n",
       "         [0.2200, 0.5800, 0.3300],\n",
       "         [0.7700, 0.2500, 0.1000],\n",
       "         [0.0500, 0.8000, 0.5500]]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2ff1bd5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1610,  0.0789],\n",
       "         [-0.1517,  0.0744],\n",
       "         [-0.3697, -0.1022],\n",
       "         [-0.4923, -0.0251],\n",
       "         [-0.6918, -0.1094],\n",
       "         [-0.8427, -0.3002]],\n",
       "\n",
       "        [[ 0.0000,  0.0000],\n",
       "         [-0.4459, -0.0064],\n",
       "         [-0.5214, -0.0278],\n",
       "         [-0.8765, -0.2752],\n",
       "         [-0.7949, -0.1041],\n",
       "         [-1.1554, -0.2665]]], grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ca(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295592f0",
   "metadata": {},
   "source": [
    "**Stacking Multiple single attention to make multi head attention**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "c21129f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#what we implemented above is a single head attention mechanism but in a multi head attention mechanism we have multiple attention heads and each attention head has its own query, key and value matrices\n",
    "#so we will stack multiple single head attention mechanisms to make a multi head attention mechanism\n",
    "\n",
    "\n",
    "class MultiHeadAttention(torch.nn.Module):\n",
    "    def __init__(self, d_in, d_out, num_heads, dropout, context_length):\n",
    "        super().__init__()\n",
    "        self.heads = torch.nn.ModuleList([CausalAttention(d_in=d_in, d_out=d_out, context_length=6, dropout=0.5) for _ in range(num_heads)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([head(x) for head in self.heads], dim=-1) #dim=-1 means that we are concatenating the output of the heads along the last dimension\n",
    "        return out\n",
    "\n",
    "\n",
    "# what we are doing here is we are stacking multiple single head attention mechanisms to make a multi head attention mechanism\n",
    "# so we are creating a list of causal attention mechanisms and we are stacking them up to make a multi head attention mechanism\n",
    "\n",
    "context_length = batch.shape[1]\n",
    "d_in, d_out = batch.shape[-1], 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "bd8cc797",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 2, 6)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_in, d_out, context_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "2f8a87a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 4])\n"
     ]
    }
   ],
   "source": [
    "mha = MultiHeadAttention(d_in, d_out, context_length=context_length, dropout=0, num_heads=2)\n",
    "context_vec = mha(batch)\n",
    "print(context_vec.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "9cbc24a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0000,  0.0000, -0.0364,  0.0342],\n",
       "         [-0.0183,  0.0773, -0.0976,  0.0848],\n",
       "         [-0.1362,  0.1180, -0.1617,  0.1407],\n",
       "         [-0.3344,  0.1102, -0.0535,  0.0437],\n",
       "         [ 0.0000,  0.0000, -0.1070,  0.0905],\n",
       "         [-0.5707,  0.2531,  0.0542,  0.0884]],\n",
       "\n",
       "        [[ 0.0000,  0.0000,  0.0000,  0.0000],\n",
       "         [-0.0945,  0.0350, -0.0976,  0.0848],\n",
       "         [-0.0182,  0.0773, -0.0972,  0.0844],\n",
       "         [-0.1174,  0.0404, -0.1050,  0.0757],\n",
       "         [-0.4025,  0.0223,  0.0488,  0.0912],\n",
       "         [-0.5957,  0.3066, -0.0634,  0.1779]]], grad_fn=<CatBackward0>)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "ac2eee6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.1029,  0.1751, -0.0969,  0.0276,  0.1675,  0.1001],\n",
      "         [ 0.2526,  0.4133, -0.1646,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000, -0.0072,  0.6432,  0.2947],\n",
      "         [ 0.3149,  0.5051, -0.1411,  0.0090,  0.7815,  0.3809],\n",
      "         [ 0.4092,  0.6672, -0.2384, -0.0188,  0.8696,  0.4006],\n",
      "         [ 0.2921,  0.4824, -0.1905, -0.0576,  1.9283,  0.8964]],\n",
      "\n",
      "        [[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.2526,  0.4133, -0.1646,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.2523,  0.4127, -0.1645,  0.0206,  0.4314,  0.2188],\n",
      "         [ 0.2595,  0.4251, -0.1714, -0.0075,  0.6650,  0.3046],\n",
      "         [ 0.3920,  0.8400, -0.4137, -0.0150,  0.5256,  0.2440],\n",
      "         [ 0.7320,  0.8764, -0.0160, -0.0583,  1.4093,  0.7391]]],\n",
      "       grad_fn=<CatBackward0>)\n",
      "context_vecs.shape: torch.Size([2, 6, 6])\n"
     ]
    }
   ],
   "source": [
    "class MultiHeadAttentionv2(torch.nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert (d_out % num_heads == 0), \\\n",
    "            \"d_out must be divisible by num_heads\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads # Reduce the projection dim to match desired output dim\n",
    "\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\n",
    "            \"mask\",\n",
    "            torch.triu(torch.ones(context_length, context_length),\n",
    "                       diagonal=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "        # As in `CausalAttention`, for inputs where `num_tokens` exceeds `context_length`, \n",
    "        # this will result in errors in the mask creation further below. \n",
    "        # In practice, this is not a problem since the LLM (chapters 4-7) ensures that inputs  \n",
    "        # do not exceed `context_length` before reaching this forward method.\n",
    "\n",
    "        keys = self.W_key(x) # Shape: (b, num_tokens, d_out)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        # We implicitly split the matrix by adding a `num_heads` dimension\n",
    "        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim) \n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n",
    "        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
    "\n",
    "        # Original mask truncated to the number of tokens and converted to boolean\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "\n",
    "        # Use the mask to fill attention scores\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "        \n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # Shape: (b, num_tokens, num_heads, head_dim)\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2) \n",
    "        \n",
    "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
    "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
    "        context_vec = self.out_proj(context_vec) # optional projection\n",
    "\n",
    "        return context_vec\n",
    "\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "batch_size, context_length, d_in = batch.shape\n",
    "d_out = 3\n",
    "mha = MultiHeadAttention(d_in=d_in, d_out=d_out, context_length=context_length, dropout=0.0, num_heads=2)\n",
    "\n",
    "context_vecs = mha(batch)\n",
    "\n",
    "print(context_vecs)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "d5fa3829",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 6, 3])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950dcf93",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
